{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b578bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "# Assuming other necessary imports like retry, types, etc. are present\n",
    "\n",
    "class DocumentEmbedding(EmbeddingFunction):\n",
    "    \"\"\"\n",
    "    Base class for embedding and retrieving documents using ChromaDB and GenAI embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chroma_client, genai_client, model_name=\"models/text-embedding-004\", collection_name=\"embeddings\"):\n",
    "        self.chroma_client = chroma_client\n",
    "        self.genai_client = genai_client\n",
    "        self.collection = self.chroma_client.get_or_create_collection(name=collection_name)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def _embed_batch(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Helper function to embed a BATCH of texts at once.\n",
    "        Returns a list of embeddings (list of list of floats).\n",
    "        \"\"\"\n",
    "        # Google API supports embedding multiple documents in one request\n",
    "        response = self.genai_client.models.embed_content(\n",
    "            model=self.model_name,\n",
    "            contents=texts,\n",
    "            config=types.EmbedContentConfig(task_type=\"retrieval_document\")\n",
    "        )\n",
    "        \n",
    "        # Extract the values from each embedding object in the response\n",
    "        return [emb.values for emb in response.embeddings]\n",
    "\n",
    "    @retry.Retry(predicate=is_retriable, timeout=1000)\n",
    "    def add_document_embeddings(self, documents, metadata, batch_size=100):\n",
    "        \"\"\"\n",
    "        Embeds a list of documents in BATCHES and stores them in ChromaDB.\n",
    "        \"\"\"\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # --- BATCHING LOGIC START ---\n",
    "        total_docs = len(documents)\n",
    "        print(f\"Starting embedding for {total_docs} documents...\")\n",
    "\n",
    "        for i in range(0, total_docs, batch_size):\n",
    "            # 1. Slice the documents into a batch (e.g., 0 to 100)\n",
    "            batch_docs = documents[i : i + batch_size]\n",
    "            \n",
    "            # 2. Get embeddings for this batch (1 API call instead of 100)\n",
    "            print(f\"Processing batch {i} to {i + len(batch_docs)}...\")\n",
    "            try:\n",
    "                batch_embeddings = self._embed_batch(batch_docs)\n",
    "                all_embeddings.extend(batch_embeddings)\n",
    "                \n",
    "                # Good practice: sleep slightly between batches to be nice to the rate limiter\n",
    "                time.sleep(0.5) \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch starting at index {i}: {e}\")\n",
    "                # You might want to raise here or handle partial failures depending on your needs\n",
    "                raise e\n",
    "        # --- BATCHING LOGIC END ---\n",
    "\n",
    "        # 3. Add everything to ChromaDB at once\n",
    "        self.collection.add(\n",
    "            documents=documents,\n",
    "            metadatas=metadata,\n",
    "            embeddings=all_embeddings,\n",
    "            ids=[str(meta[self.id_field]) for meta in metadata]\n",
    "        )\n",
    "        print(\"All documents added successfully.\")\n",
    "\n",
    "    def query_embedding(self, query, n_results=5):\n",
    "        \"\"\"\n",
    "        Embeds the input query (single string) and searches ChromaDB.\n",
    "        \"\"\"\n",
    "        # For a single query, we just wrap it in a list to use the same logic, \n",
    "        # but we need \"retrieval_query\" task type for better accuracy.\n",
    "        response = self.genai_client.models.embed_content(\n",
    "            model=self.model_name,\n",
    "            contents=[query],\n",
    "            config=types.EmbedContentConfig(task_type=\"retrieval_query\") # Note the task type change\n",
    "        )\n",
    "        \n",
    "        embedded_query = response.embeddings[0].values\n",
    "\n",
    "        return self.collection.query(\n",
    "            query_embeddings=[embedded_query],\n",
    "            n_results=n_results\n",
    "        ) if embedded_query else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7532f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class LightWeightRAG:\n",
    "    def __init__(self, db_path=\"rag_data.db\", dimension=768):\n",
    "        self.db_path = db_path\n",
    "        self.dimension = dimension\n",
    "        \n",
    "        # 1. Setup SQLite (The Content Store)\n",
    "        self.conn = sqlite3.connect(self.db_path, check_same_thread=False)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self._init_db()\n",
    "\n",
    "        # 2. Setup FAISS (The Vector Index)\n",
    "        # We use IndexIDMap so we can assign specific SQLite IDs to vectors\n",
    "        self.index = faiss.IndexIDMap(faiss.IndexFlatL2(self.dimension))\n",
    "\n",
    "    def _init_db(self):\n",
    "        \"\"\"Creates the table if it doesn't exist.\"\"\"\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS abstracts (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                paper_id TEXT,\n",
    "                title TEXT,\n",
    "                date TEXT,\n",
    "                content TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    def add_documents(self, documents, embeddings):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            documents: List of dicts [{'paper_id': '...', 'title': '...', 'content': '...'}, ...]\n",
    "            embeddings: List of list of floats (from Google GenAI)\n",
    "        \"\"\"\n",
    "        # Step A: Insert text into SQLite to get unique IDs\n",
    "        ids_list = []\n",
    "        for doc in documents:\n",
    "            self.cursor.execute(\"\"\"\n",
    "                INSERT INTO abstracts (paper_id, title, date, content)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "            \"\"\", (doc.get('paper_id'), doc.get('title'), doc.get('date'), doc.get('content')))\n",
    "            \n",
    "            # Capture the auto-incremented ID generated by SQLite\n",
    "            ids_list.append(self.cursor.lastrowid)\n",
    "        \n",
    "        self.conn.commit()\n",
    "\n",
    "        # Step B: Add vectors to FAISS using the SAME IDs\n",
    "        # FAISS requires float32 numpy arrays\n",
    "        vectors_np = np.array(embeddings).astype('float32')\n",
    "        ids_np = np.array(ids_list).astype('int64')\n",
    "        \n",
    "        self.index.add_with_ids(vectors_np, ids_np)\n",
    "        print(f\"Successfully stored {len(documents)} documents.\")\n",
    "\n",
    "    def search(self, query_embedding, k=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query_embedding: List of floats (single query vector)\n",
    "            k: Number of results to return\n",
    "        \"\"\"\n",
    "        # Step A: Search FAISS for the closest vectors\n",
    "        query_np = np.array([query_embedding]).astype('float32')\n",
    "        distances, indices = self.index.search(query_np, k)\n",
    "        \n",
    "        # 'indices' contains the IDs of the matches (e.g., [5, 23, 101])\n",
    "        found_ids = indices[0]\n",
    "        \n",
    "        # Filter out -1 (FAISS returns -1 if fewer than k results exist)\n",
    "        valid_ids = [int(i) for i in found_ids if i != -1]\n",
    "        \n",
    "        if not valid_ids:\n",
    "            return []\n",
    "\n",
    "        # Step B: Fetch the actual text from SQLite using those IDs\n",
    "        # Construct dynamic SQL query: \"SELECT * FROM abstracts WHERE id IN (?,?,?)\"\n",
    "        placeholders = ','.join('?' * len(valid_ids))\n",
    "        query = f\"SELECT paper_id, title, content FROM abstracts WHERE id IN ({placeholders})\"\n",
    "        \n",
    "        self.cursor.execute(query, valid_ids)\n",
    "        results = self.cursor.fetchall()\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def close(self):\n",
    "        self.conn.close()\n",
    "\n",
    "# --- Usage Example ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize\n",
    "    rag = LightWeightRAG(db_path=\"my_papers.db\")\n",
    "\n",
    "    # 1. Mock Data (Your Google GenAI results go here)\n",
    "    mock_docs = [\n",
    "        {\"paper_id\": \"A1\", \"title\": \"Deep Learning\", \"date\": \"2023\", \"content\": \"Abstract about DL...\"},\n",
    "        {\"paper_id\": \"B2\", \"title\": \"Quantum Comp\", \"date\": \"2024\", \"content\": \"Abstract about QC...\"}\n",
    "    ]\n",
    "    # Mock Embeddings (768 dimensions)\n",
    "    mock_embeddings = [np.random.rand(768), np.random.rand(768)]\n",
    "\n",
    "    # 2. Add to DB\n",
    "    rag.add_documents(mock_docs, mock_embeddings)\n",
    "\n",
    "    # 3. Search\n",
    "    # (In real life, this comes from embed_content(\"search query\"))\n",
    "    mock_query = np.random.rand(768) \n",
    "    results = rag.search(mock_query, k=1)\n",
    "\n",
    "    print(\"Retrieved Result:\", results)\n",
    "    rag.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(pgdbda)",
   "language": "python",
   "name": "pgdbda"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
